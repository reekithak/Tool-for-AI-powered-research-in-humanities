# -*- coding: utf-8 -*-
"""FINAL NLP HUMANITIIES.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HDQsLNq01D36EHprDHwFZ_dCUJA3Uk9Q

**Bert-Model**
"""

from google.colab import drive

pip install bert-extractive-summarizer

from summarizer import Summarizer

drive.mount('/content/drive')

mylines = []   
file = 'drive/My Drive/final.txt'

'''with open (file, 'rt') as myfile:
    for myline in myfile:                
        mylines.append(myline)'''

with open (file, 'rt') as myfile:
    mylines = myfile.read().split('.')

newm = []
for line in mylines:
  temp = line.replace("\n",'')
  newm.append(temp)

newm

#print("Enter the keywords related to your research topic? , Enter No to stop the inputs")
wrd_list = []
x='yes'
while x.lower()!="no":
    usr_input = input("Enter the keywords related to your research topic: ")
    wrd_list.append(usr_input)
    print("Do You wish to continue to enter another keyword?")
    x = input()

file = open("drive/My Drive/copy.txt", "w")     #new extracted file will seve in copy.txt file
iter_= 1
for i in newm:
     for wrd in wrd_list:
        
         if wrd in i.lower():
            print(wrd)
            file.write(i) 
            print("Recognised "+str(iter_))
            iter_+=1
file.close()

new=[]
with open ('drive/My Drive/copy.txt', 'rt') as txtfile: 
    for line in txtfile:               
        new.append(line)

txt = ''.join(str(i) for i in new)
txt

"""**Bert Extractive Summary**"""

model = Summarizer()

res = model(txt,min_length=60)
fin = ''.join(res)
fin

"""**PY Summarizer - LSTM (seq2seq)**"""

pip install pysummarization

from pysummarization.nlpbase.auto_abstractor import AutoAbstractor
from pysummarization.tokenizabledoc.simple_tokenizer import SimpleTokenizer
from pysummarization.abstractabledoc.top_n_rank_abstractor import TopNRankAbstractor

txt

auto_ab = AutoAbstractor()
auto_ab.tokenizable_doc = SimpleTokenizer()
auto_ab.delimiter_list = [".", "\n"]
abstractable_doc = TopNRankAbstractor()
result_dict = auto_ab.summarize(txt, abstractable_doc)
restxt = str(result_dict['summarize_result'])
try:
    restxt = restxt.replace('[','') ; restxt = restxt.replace(']','') ; restxt = restxt.replace('\\n','') ; restxt = restxt.replace("'",'')
except:
    pass

"""**PY Summary - LSTM (seq2seq)**"""

summ = [str(x) for x in restxt.split('.')]
msumm=[]
for l in summ:
    l = l.replace(',',''); #l =l.replace(" ","")
    
    msumm.append(l)
finstr = '.'.join(str(i) for i in msumm) + '.'
finstr

"""**NLTK Summarizer**"""

pip install text-summarizer

from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize, sent_tokenize 
import nltk
nltk.download('stopwords')
nltk.download('punkt')

stopWords = set(stopwords.words("english")) 
words = word_tokenize(txt) 
freqTable = dict()

freqTable = dict() 
for word in words: 
    word = word.lower() 
    if word in stopWords: 
        continue
    if word in freqTable: 
        freqTable[word] += 1
    else: 
        freqTable[word] = 1

sentences = sent_tokenize(txt) 
sentenceValue = dict()

for sentence in sentences: 
    for word, freq in freqTable.items(): 
        if word in sentence.lower(): 
            if sentence in sentenceValue: 
                sentenceValue[sentence] += freq 
            else: 
                sentenceValue[sentence] = freq

sumValues = 0
for sentence in sentenceValue: 
    sumValues += sentenceValue[sentence]

try:
  average = int(sumValues / len(sentenceValue)) 
except ZeroDivisionError:
  average= 0

"""**NLTK Summary**"""

summary = '' 
for sentence in sentences: 
    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)): 
        summary += " " + sentence 
summary

